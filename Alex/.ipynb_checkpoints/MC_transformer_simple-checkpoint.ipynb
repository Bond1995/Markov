{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd5732bd-3e09-4711-b204-cd97f4930a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47698e8c-0200-4ce8-8675-991422afc7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transition matrix for visible states\n",
    "P = torch.zeros(2,2,2)\n",
    "\n",
    "p_G, p_B = 0.8, 0.3\n",
    "#p_G, p_B = 0, 1\n",
    "P[0,:,:] = torch.Tensor([[1-p_G,p_G],[p_G,1-p_G]])\n",
    "P[1,:,:] = torch.Tensor([[1-p_B,p_B],[p_B,1-p_B]])\n",
    "\n",
    "p_G, p_B = 0.8, 0.3\n",
    "#p_G, p_B = 0, 1\n",
    "P[0,:,:] = torch.Tensor([[1-0.2,0.2],[0.3,0.7]])\n",
    "P[1,:,:] = torch.Tensor([[1-p_B,p_B],[p_B,1-p_B]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b5ed886-01ea-4632-a6a1-fadd9b791783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transition matrix for hidden error process\n",
    "#b, g = 0., 0.7 # real\n",
    "b, g = 0.1, 0.7 # real 2\n",
    "b, g = 0.1, 0.5 # real 3\n",
    "b, g = 0, 1 # real 4\n",
    "#b, g = 0.2, 0.2\n",
    "#b, g = 0, 1\n",
    "P_hidden = torch.Tensor([[1-b,b],[g,1-g]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "151574b2-1d16-4b67-81d0-6c17a7d87204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_symbols(P, P_hidden, data, data_hidden):\n",
    "    M = P[data_hidden.to(int),data.to(int)]\n",
    "    s = torch.multinomial(M,1).flatten()\n",
    "\n",
    "    M_hidden = P_hidden[data_hidden.to(int)]\n",
    "    s_hidden = torch.multinomial(M_hidden,1).flatten()\n",
    "\n",
    "\n",
    "    return s, s_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa9d934b-1d20-45bd-b27b-2eeaeaebb968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(P, P_hidden, seq_length, batch_size):\n",
    "    #alpha = 0.5\n",
    "    alpha = 0 # real 4\n",
    "    data = torch.zeros(batch_size, seq_length+1)\n",
    "    data[:,0] = torch.bernoulli(alpha*torch.ones((batch_size,)))\n",
    "    data_hidden = torch.zeros(batch_size, seq_length+1)\n",
    "    data_hidden[:,0] = torch.bernoulli(alpha*torch.ones((batch_size,)))\n",
    "    for i in range(seq_length):\n",
    "        data[:,i+1], data_hidden[:,i+1] = get_next_symbols(P, P_hidden, data[:,i], data_hidden[:,i])\n",
    "    x = data[:,:seq_length].to(int)\n",
    "    y = data[:,1:].to(int)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63ec9ef0-f901-4d7a-b767-18af13ebf921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "          1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "          0, 1],\n",
       "         [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          1, 1]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "          1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "          1, 1],\n",
       "         [0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "          1, 1]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batch(P, P_hidden, 50, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaed4c04-7c5d-421b-925d-ca5f3c353ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import GPTBase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eef50e-4cd4-4dd4-8a9d-0b8b165d9f98",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cd40200-bfdf-4fcb-bba2-d8c5a96016fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    n_embd = 8,#n_embd = 32,\n",
    "    sequence_length = 1024,\n",
    "    bias = False,\n",
    "    dropout = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a97e98dd-a09a-433d-8cee-1f199d92944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = SimpleNamespace(\n",
    "    iterations = 8000,\n",
    "    beta1 = 0.9,\n",
    "    beta2 = 0.95,\n",
    "    lr = 2e-3,\n",
    "    batch_size = 16,\n",
    "    weight_decay = 1e-3,\n",
    "    scheduler = 'cos',\n",
    "    warmup_percent = 0.02,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f478aa4-eb8f-420f-ae42-492558039684",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval(model, P, P_hidden, sequence_length, batch_size, max_num_batches=20):\n",
    "    assert model.training == False\n",
    "\n",
    "    loss_list_val, acc_list = [], []\n",
    "\n",
    "    for _ in range(max_num_batches): \n",
    "        x, y = get_batch(P, P_hidden, sequence_length, batch_size)\n",
    "        outputs, _ = model(x, y)\n",
    "        outputs = outputs.squeeze(-1)\n",
    "        val_loss = F.binary_cross_entropy_with_logits(outputs.view(-1), y.float().view(-1))\n",
    "        loss_list_val.append(val_loss)\n",
    "        acc_list.append(((outputs > 0) == y.to(bool)).float().mean())\n",
    "\n",
    "    val_acc = torch.stack(acc_list).mean().item()\n",
    "    val_loss = torch.stack(loss_list_val).mean().item()\n",
    "    val_perplexity = 2.71828 ** val_loss\n",
    "\n",
    "    return val_acc, val_loss, val_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6f6dea-ee2e-439f-9bf4-d252a76f0611",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pumping\n",
      "1 [train] loss=0.69283 [val] loss=0.69282, pp=2.00, acc=0.571716 [time per itr] 298.61ms [lr] 0.00002\n",
      "2 [train] loss=0.69281 [val] loss=0.69282, pp=2.00, acc=0.576080 [time per itr] 227.01ms [lr] 0.00002\n",
      "3 [train] loss=0.69281 [val] loss=0.69282, pp=2.00, acc=0.576221 [time per itr] 248.45ms [lr] 0.00002\n",
      "4 [train] loss=0.69282 [val] loss=0.69282, pp=2.00, acc=0.577307 [time per itr] 220.38ms [lr] 0.00002\n",
      "5 [train] loss=0.69279 [val] loss=0.69282, pp=2.00, acc=0.580170 [time per itr] 217.22ms [lr] 0.00002\n",
      "6 [train] loss=0.69282 [val] loss=0.69281, pp=2.00, acc=0.586243 [time per itr] 222.26ms [lr] 0.00003\n",
      "7 [train] loss=0.69282 [val] loss=0.69282, pp=2.00, acc=0.583759 [time per itr] 252.52ms [lr] 0.00003\n",
      "8 [train] loss=0.69283 [val] loss=0.69281, pp=2.00, acc=0.588171 [time per itr] 246.55ms [lr] 0.00003\n",
      "9 [train] loss=0.69280 [val] loss=0.69281, pp=2.00, acc=0.593042 [time per itr] 220.07ms [lr] 0.00004\n",
      "10 [train] loss=0.69281 [val] loss=0.69279, pp=2.00, acc=0.603754 [time per itr] 220.95ms [lr] 0.00004\n",
      "11 [train] loss=0.69281 [val] loss=0.69280, pp=2.00, acc=0.604449 [time per itr] 218.55ms [lr] 0.00004\n",
      "12 [train] loss=0.69277 [val] loss=0.69279, pp=2.00, acc=0.609943 [time per itr] 216.86ms [lr] 0.00005\n",
      "13 [train] loss=0.69279 [val] loss=0.69278, pp=2.00, acc=0.616974 [time per itr] 218.00ms [lr] 0.00005\n",
      "14 [train] loss=0.69278 [val] loss=0.69277, pp=2.00, acc=0.622882 [time per itr] 219.74ms [lr] 0.00006\n",
      "15 [train] loss=0.69276 [val] loss=0.69277, pp=2.00, acc=0.624969 [time per itr] 220.61ms [lr] 0.00006\n",
      "16 [train] loss=0.69275 [val] loss=0.69276, pp=2.00, acc=0.633429 [time per itr] 247.70ms [lr] 0.00007\n",
      "17 [train] loss=0.69277 [val] loss=0.69275, pp=2.00, acc=0.642755 [time per itr] 218.20ms [lr] 0.00008\n",
      "18 [train] loss=0.69276 [val] loss=0.69274, pp=2.00, acc=0.651477 [time per itr] 220.98ms [lr] 0.00008\n",
      "19 [train] loss=0.69271 [val] loss=0.69272, pp=2.00, acc=0.659906 [time per itr] 225.92ms [lr] 0.00009\n",
      "20 [train] loss=0.69274 [val] loss=0.69272, pp=2.00, acc=0.662738 [time per itr] 219.77ms [lr] 0.00010\n",
      "21 [train] loss=0.69273 [val] loss=0.69270, pp=2.00, acc=0.675446 [time per itr] 220.63ms [lr] 0.00010\n",
      "22 [train] loss=0.69269 [val] loss=0.69268, pp=2.00, acc=0.686566 [time per itr] 219.98ms [lr] 0.00011\n",
      "23 [train] loss=0.69267 [val] loss=0.69267, pp=2.00, acc=0.693665 [time per itr] 217.52ms [lr] 0.00012\n",
      "24 [train] loss=0.69266 [val] loss=0.69265, pp=2.00, acc=0.704675 [time per itr] 244.71ms [lr] 0.00013\n",
      "25 [train] loss=0.69265 [val] loss=0.69263, pp=2.00, acc=0.710144 [time per itr] 225.23ms [lr] 0.00014\n",
      "26 [train] loss=0.69264 [val] loss=0.69261, pp=2.00, acc=0.718774 [time per itr] 219.07ms [lr] 0.00015\n",
      "27 [train] loss=0.69262 [val] loss=0.69260, pp=2.00, acc=0.721869 [time per itr] 219.43ms [lr] 0.00016\n",
      "28 [train] loss=0.69259 [val] loss=0.69257, pp=2.00, acc=0.730121 [time per itr] 219.56ms [lr] 0.00017\n",
      "29 [train] loss=0.69256 [val] loss=0.69255, pp=2.00, acc=0.731732 [time per itr] 220.92ms [lr] 0.00018\n",
      "30 [train] loss=0.69254 [val] loss=0.69253, pp=2.00, acc=0.736047 [time per itr] 219.33ms [lr] 0.00019\n",
      "31 [train] loss=0.69252 [val] loss=0.69249, pp=2.00, acc=0.741486 [time per itr] 214.81ms [lr] 0.00020\n",
      "32 [train] loss=0.69250 [val] loss=0.69247, pp=2.00, acc=0.740234 [time per itr] 217.69ms [lr] 0.00021\n",
      "33 [train] loss=0.69248 [val] loss=0.69244, pp=2.00, acc=0.740686 [time per itr] 218.36ms [lr] 0.00022\n",
      "34 [train] loss=0.69244 [val] loss=0.69240, pp=2.00, acc=0.740027 [time per itr] 223.32ms [lr] 0.00024\n",
      "35 [train] loss=0.69240 [val] loss=0.69238, pp=2.00, acc=0.735449 [time per itr] 224.07ms [lr] 0.00025\n",
      "36 [train] loss=0.69237 [val] loss=0.69234, pp=2.00, acc=0.730768 [time per itr] 244.51ms [lr] 0.00026\n",
      "37 [train] loss=0.69233 [val] loss=0.69230, pp=2.00, acc=0.723877 [time per itr] 222.50ms [lr] 0.00027\n",
      "38 [train] loss=0.69230 [val] loss=0.69226, pp=2.00, acc=0.719836 [time per itr] 212.73ms [lr] 0.00029\n",
      "39 [train] loss=0.69225 [val] loss=0.69222, pp=2.00, acc=0.711023 [time per itr] 213.37ms [lr] 0.00030\n",
      "40 [train] loss=0.69220 [val] loss=0.69217, pp=2.00, acc=0.703961 [time per itr] 221.59ms [lr] 0.00031\n",
      "41 [train] loss=0.69216 [val] loss=0.69213, pp=2.00, acc=0.691840 [time per itr] 219.06ms [lr] 0.00033\n",
      "42 [train] loss=0.69214 [val] loss=0.69209, pp=2.00, acc=0.681445 [time per itr] 221.71ms [lr] 0.00034\n",
      "43 [train] loss=0.69207 [val] loss=0.69202, pp=2.00, acc=0.672137 [time per itr] 217.16ms [lr] 0.00036\n",
      "44 [train] loss=0.69202 [val] loss=0.69197, pp=2.00, acc=0.663812 [time per itr] 226.84ms [lr] 0.00037\n",
      "45 [train] loss=0.69199 [val] loss=0.69191, pp=2.00, acc=0.653754 [time per itr] 219.95ms [lr] 0.00039\n",
      "46 [train] loss=0.69188 [val] loss=0.69186, pp=2.00, acc=0.643341 [time per itr] 225.51ms [lr] 0.00040\n",
      "47 [train] loss=0.69187 [val] loss=0.69180, pp=2.00, acc=0.635803 [time per itr] 216.71ms [lr] 0.00042\n",
      "48 [train] loss=0.69182 [val] loss=0.69174, pp=2.00, acc=0.626318 [time per itr] 220.42ms [lr] 0.00043\n",
      "49 [train] loss=0.69168 [val] loss=0.69165, pp=2.00, acc=0.623065 [time per itr] 224.87ms [lr] 0.00045\n",
      "50 [train] loss=0.69168 [val] loss=0.69160, pp=2.00, acc=0.614307 [time per itr] 218.90ms [lr] 0.00047\n",
      "51 [train] loss=0.69165 [val] loss=0.69150, pp=2.00, acc=0.615027 [time per itr] 217.01ms [lr] 0.00048\n",
      "52 [train] loss=0.69148 [val] loss=0.69143, pp=2.00, acc=0.611017 [time per itr] 221.00ms [lr] 0.00050\n",
      "53 [train] loss=0.69139 [val] loss=0.69134, pp=2.00, acc=0.609564 [time per itr] 248.17ms [lr] 0.00052\n",
      "54 [train] loss=0.69124 [val] loss=0.69128, pp=2.00, acc=0.604919 [time per itr] 246.09ms [lr] 0.00053\n",
      "55 [train] loss=0.69126 [val] loss=0.69120, pp=2.00, acc=0.600867 [time per itr] 245.26ms [lr] 0.00055\n",
      "56 [train] loss=0.69116 [val] loss=0.69106, pp=2.00, acc=0.606262 [time per itr] 212.90ms [lr] 0.00057\n",
      "57 [train] loss=0.69115 [val] loss=0.69104, pp=2.00, acc=0.598358 [time per itr] 245.53ms [lr] 0.00058\n",
      "58 [train] loss=0.69094 [val] loss=0.69088, pp=2.00, acc=0.603424 [time per itr] 237.12ms [lr] 0.00060\n",
      "59 [train] loss=0.69077 [val] loss=0.69081, pp=2.00, acc=0.599115 [time per itr] 243.99ms [lr] 0.00062\n",
      "60 [train] loss=0.69081 [val] loss=0.69065, pp=2.00, acc=0.603522 [time per itr] 247.33ms [lr] 0.00064\n",
      "61 [train] loss=0.69071 [val] loss=0.69056, pp=1.99, acc=0.601044 [time per itr] 216.02ms [lr] 0.00066\n",
      "62 [train] loss=0.69038 [val] loss=0.69044, pp=1.99, acc=0.600867 [time per itr] 247.43ms [lr] 0.00067\n",
      "63 [train] loss=0.69039 [val] loss=0.69034, pp=1.99, acc=0.599353 [time per itr] 223.01ms [lr] 0.00069\n",
      "64 [train] loss=0.69038 [val] loss=0.69019, pp=1.99, acc=0.600317 [time per itr] 221.12ms [lr] 0.00071\n",
      "65 [train] loss=0.69010 [val] loss=0.69007, pp=1.99, acc=0.599481 [time per itr] 242.23ms [lr] 0.00073\n",
      "66 [train] loss=0.69005 [val] loss=0.68989, pp=1.99, acc=0.601660 [time per itr] 215.74ms [lr] 0.00075\n",
      "67 [train] loss=0.68996 [val] loss=0.68979, pp=1.99, acc=0.599457 [time per itr] 213.94ms [lr] 0.00077\n",
      "68 [train] loss=0.68982 [val] loss=0.68965, pp=1.99, acc=0.598853 [time per itr] 214.86ms [lr] 0.00079\n",
      "69 [train] loss=0.68951 [val] loss=0.68947, pp=1.99, acc=0.599536 [time per itr] 242.81ms [lr] 0.00081\n",
      "70 [train] loss=0.68939 [val] loss=0.68930, pp=1.99, acc=0.600024 [time per itr] 245.40ms [lr] 0.00083\n",
      "71 [train] loss=0.68940 [val] loss=0.68914, pp=1.99, acc=0.599817 [time per itr] 243.61ms [lr] 0.00084\n",
      "72 [train] loss=0.68922 [val] loss=0.68890, pp=1.99, acc=0.603107 [time per itr] 263.25ms [lr] 0.00086\n",
      "73 [train] loss=0.68894 [val] loss=0.68876, pp=1.99, acc=0.599902 [time per itr] 223.13ms [lr] 0.00088\n",
      "74 [train] loss=0.68876 [val] loss=0.68854, pp=1.99, acc=0.601385 [time per itr] 213.90ms [lr] 0.00090\n",
      "75 [train] loss=0.68854 [val] loss=0.68830, pp=1.99, acc=0.603223 [time per itr] 215.00ms [lr] 0.00092\n",
      "76 [train] loss=0.68830 [val] loss=0.68815, pp=1.99, acc=0.599567 [time per itr] 215.31ms [lr] 0.00094\n",
      "77 [train] loss=0.68793 [val] loss=0.68795, pp=1.99, acc=0.599860 [time per itr] 214.83ms [lr] 0.00096\n",
      "78 [train] loss=0.68797 [val] loss=0.68769, pp=1.99, acc=0.600653 [time per itr] 208.21ms [lr] 0.00098\n",
      "79 [train] loss=0.68771 [val] loss=0.68747, pp=1.99, acc=0.600714 [time per itr] 248.58ms [lr] 0.00100\n",
      "80 [train] loss=0.68746 [val] loss=0.68721, pp=1.99, acc=0.600452 [time per itr] 224.39ms [lr] 0.00102\n",
      "81 [train] loss=0.68714 [val] loss=0.68697, pp=1.99, acc=0.599048 [time per itr] 215.61ms [lr] 0.00104\n",
      "82 [train] loss=0.68694 [val] loss=0.68668, pp=1.99, acc=0.599420 [time per itr] 217.07ms [lr] 0.00106\n",
      "83 [train] loss=0.68647 [val] loss=0.68633, pp=1.99, acc=0.601019 [time per itr] 216.24ms [lr] 0.00108\n",
      "84 [train] loss=0.68643 [val] loss=0.68605, pp=1.99, acc=0.601630 [time per itr] 215.64ms [lr] 0.00110\n",
      "85 [train] loss=0.68630 [val] loss=0.68564, pp=1.99, acc=0.604144 [time per itr] 218.17ms [lr] 0.00112\n",
      "86 [train] loss=0.68543 [val] loss=0.68545, pp=1.98, acc=0.600189 [time per itr] 213.30ms [lr] 0.00114\n",
      "87 [train] loss=0.68547 [val] loss=0.68502, pp=1.98, acc=0.602124 [time per itr] 245.50ms [lr] 0.00116\n",
      "88 [train] loss=0.68502 [val] loss=0.68465, pp=1.98, acc=0.601355 [time per itr] 217.60ms [lr] 0.00118\n",
      "89 [train] loss=0.68458 [val] loss=0.68429, pp=1.98, acc=0.601001 [time per itr] 216.21ms [lr] 0.00119\n",
      "90 [train] loss=0.68451 [val] loss=0.68399, pp=1.98, acc=0.599536 [time per itr] 212.26ms [lr] 0.00121\n",
      "91 [train] loss=0.68404 [val] loss=0.68350, pp=1.98, acc=0.601855 [time per itr] 215.21ms [lr] 0.00123\n",
      "92 [train] loss=0.68363 [val] loss=0.68309, pp=1.98, acc=0.600421 [time per itr] 218.13ms [lr] 0.00125\n",
      "93 [train] loss=0.68248 [val] loss=0.68259, pp=1.98, acc=0.600494 [time per itr] 214.07ms [lr] 0.00127\n",
      "94 [train] loss=0.68268 [val] loss=0.68218, pp=1.98, acc=0.600061 [time per itr] 209.78ms [lr] 0.00129\n",
      "95 [train] loss=0.68259 [val] loss=0.68141, pp=1.98, acc=0.606177 [time per itr] 223.17ms [lr] 0.00131\n",
      "96 [train] loss=0.68183 [val] loss=0.68135, pp=1.98, acc=0.594745 [time per itr] 214.21ms [lr] 0.00133\n",
      "97 [train] loss=0.68144 [val] loss=0.68034, pp=1.97, acc=0.605695 [time per itr] 229.33ms [lr] 0.00135\n",
      "98 [train] loss=0.68024 [val] loss=0.68009, pp=1.97, acc=0.599310 [time per itr] 212.99ms [lr] 0.00136\n",
      "99 [train] loss=0.67963 [val] loss=0.67926, pp=1.97, acc=0.603375 [time per itr] 222.54ms [lr] 0.00138\n",
      "100 [train] loss=0.67887 [val] loss=0.67849, pp=1.97, acc=0.606592 [time per itr] 229.95ms [lr] 0.00140\n",
      "101 [train] loss=0.67828 [val] loss=0.67766, pp=1.97, acc=0.609753 [time per itr] 219.88ms [lr] 0.00142\n",
      "102 [train] loss=0.67703 [val] loss=0.67706, pp=1.97, acc=0.610901 [time per itr] 239.18ms [lr] 0.00144\n",
      "103 [train] loss=0.67723 [val] loss=0.67651, pp=1.97, acc=0.609052 [time per itr] 224.90ms [lr] 0.00145\n",
      "104 [train] loss=0.67666 [val] loss=0.67537, pp=1.96, acc=0.615698 [time per itr] 220.80ms [lr] 0.00147\n",
      "105 [train] loss=0.67640 [val] loss=0.67468, pp=1.96, acc=0.617371 [time per itr] 220.37ms [lr] 0.00149\n",
      "106 [train] loss=0.67448 [val] loss=0.67381, pp=1.96, acc=0.622449 [time per itr] 215.30ms [lr] 0.00150\n",
      "107 [train] loss=0.67431 [val] loss=0.67269, pp=1.96, acc=0.628595 [time per itr] 216.21ms [lr] 0.00152\n",
      "108 [train] loss=0.67245 [val] loss=0.67205, pp=1.96, acc=0.627411 [time per itr] 248.68ms [lr] 0.00154\n",
      "109 [train] loss=0.67113 [val] loss=0.67067, pp=1.96, acc=0.638037 [time per itr] 215.28ms [lr] 0.00155\n",
      "110 [train] loss=0.67047 [val] loss=0.66951, pp=1.95, acc=0.644739 [time per itr] 212.64ms [lr] 0.00157\n",
      "111 [train] loss=0.66905 [val] loss=0.66874, pp=1.95, acc=0.648151 [time per itr] 243.58ms [lr] 0.00159\n",
      "112 [train] loss=0.66782 [val] loss=0.66733, pp=1.95, acc=0.654712 [time per itr] 213.35ms [lr] 0.00160\n",
      "113 [train] loss=0.66696 [val] loss=0.66590, pp=1.95, acc=0.664612 [time per itr] 214.89ms [lr] 0.00162\n",
      "114 [train] loss=0.66620 [val] loss=0.66474, pp=1.94, acc=0.668451 [time per itr] 212.81ms [lr] 0.00163\n",
      "115 [train] loss=0.66531 [val] loss=0.66294, pp=1.94, acc=0.678564 [time per itr] 247.63ms [lr] 0.00165\n",
      "116 [train] loss=0.66350 [val] loss=0.66138, pp=1.94, acc=0.685498 [time per itr] 213.99ms [lr] 0.00166\n",
      "117 [train] loss=0.66111 [val] loss=0.65971, pp=1.93, acc=0.694562 [time per itr] 214.35ms [lr] 0.00168\n",
      "118 [train] loss=0.66004 [val] loss=0.65827, pp=1.93, acc=0.698596 [time per itr] 215.43ms [lr] 0.00169\n",
      "119 [train] loss=0.65828 [val] loss=0.65674, pp=1.93, acc=0.703235 [time per itr] 208.76ms [lr] 0.00171\n",
      "120 [train] loss=0.65537 [val] loss=0.65411, pp=1.92, acc=0.713214 [time per itr] 213.24ms [lr] 0.00172\n",
      "121 [train] loss=0.65583 [val] loss=0.65232, pp=1.92, acc=0.715979 [time per itr] 213.82ms [lr] 0.00173\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "itr, best_val_loss, text_table = 0, float('inf'), None\n",
    "\n",
    "stats = {'train_loss': [], 'val_loss': [], 'norm': [], 'val_acc': []}\n",
    "\n",
    "model = GPTBase(config)\n",
    "model.train()\n",
    "\n",
    "opt = optim.AdamW(model.parameters(), lr=train_args.lr, betas=(train_args.beta1, train_args.beta2),\n",
    "                  weight_decay=train_args.weight_decay)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, max_lr=train_args.lr, total_steps=train_args.iterations, \n",
    "                                               pct_start=train_args.warmup_percent, anneal_strategy=train_args.scheduler, \n",
    "                                               cycle_momentum=False, div_factor=1e2, final_div_factor=.05)\n",
    "#scheduler = None\n",
    "\n",
    "t0 = time.time()\n",
    "print('Starting pumping')\n",
    "while itr < train_args.iterations:\n",
    "    x, y = get_batch(P, P_hidden, config.sequence_length, train_args.batch_size)\n",
    "    outputs, contrib_norm = model(x)\n",
    "    outputs = outputs.squeeze(-1)\n",
    "    \n",
    "    loss = F.binary_cross_entropy_with_logits(outputs.view(-1), y.float().view(-1))\n",
    "    loss.backward()\n",
    "\n",
    "    opt.step()\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    itr += 1\n",
    "\n",
    "    stats['norm'].append(contrib_norm)\n",
    "    stats['train_loss'].append(loss.detach().cpu().item())\n",
    "\n",
    "    if itr % 1 == 0 or itr == train_args.iterations: # from here it's only evaluation code, all the training is above\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "\n",
    "        model.eval()\n",
    "        train_loss = loss.detach().cpu().item()\n",
    "        current_lr = scheduler.get_last_lr()[0] if scheduler is not None else train_args.lr\n",
    "        val_acc, val_loss, val_perplexity = eval(model, P, P_hidden, config.sequence_length, train_args.batch_size,\n",
    "                                                 max_num_batches=10)\n",
    "\n",
    "        print_string = f\"{itr} [train] loss={train_loss:.5f} [val] loss={val_loss:.5f}, pp={val_perplexity:.2f}, acc={val_acc:3f}\"\n",
    "        print_string += f\" [time per itr] {dt*1000/1:.2f}ms\"\n",
    "        if scheduler is not None:\n",
    "            print_string += f\" [lr] {current_lr:.5f}\"\n",
    "        print_string += f\"\"\n",
    "        print(print_string)\n",
    "\n",
    "        stats['val_loss'].append(val_loss)\n",
    "        stats['val_acc'].append(val_acc)\n",
    "        torch.save(stats, './stats_real_6.pt')\n",
    "\n",
    "        model.train()\n",
    "        t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250f6b5-60ca-4aed-9bee-193109929d52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae569ee-d05b-4625-93cc-f854248f2d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d32d8b-83a8-4a8d-a5ad-3b2d82403146",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
