Training model=base
{'config_format': 'markov', 'batch_size': 16, 'acc_steps': 1, 'seed': 0, 'device': device(type='cuda', index=0), 'iterations': 1000, 'lr': 0.002, 'warmup_percent': 0.02, 'weight_decay': 0.001, 'beta1': 0.9, 'beta2': 0.95, 'scheduler': 'cos', 'opt': 'adamw', 'eval_freq': 1, 'results_base_folder': './exps', 'grad_clip': 0.0, 'dataset': 'markov', 'vocab_size': 2, 'data_in_ram': False, 'model': 'base', 'use_pretrained': 'none', 'dropout': 0, 'n_head': 1, 'n_layer': 1, 'n_embd': 8, 'sequence_length': 1024, 'dtype': torch.float16, 'bias': False, 'no_compile': True, 'wandb': True, 'wandb_project': 'bias-test', 'wandb_run_prefix': 'none', 'eval_seq_prefix': '0', 'distributed_backend': None, 'p': 0.5, 'q': 0.5, 'order': 1, 'chain': 'random', 'memory': -1, 'initial': 'uniform', 'init': 'base', 'init_value': 1.0, 'world_size': 1}
9002
Markov transition matrix:
tensor([[0.2239, 0.7761],
        [0.7260, 0.2740]], device='cuda:0')
id0 x:
id0 x:
id0 x:
id0 x:
id0 x:
id0 x:
id0 x:
id0 x:
id0 x:
id0 x:
id0 x:
1 [train] loss=0.736 [val] loss=0.736, pp=2.09, acc=0.264099 [time per itr] 2296.37ms [lr] 0.00003
id0 x:
id0 x:
id0 x:
id0 x:
id0 x:
id0 x:
id0 x:
id0 x:
id0 x:
id0 x:
id0 x:
2 [train] loss=0.736 [val] loss=0.736, pp=2.09, acc=0.262317 [time per itr] 268.91ms [lr] 0.00007
id0 x:
id0 x:
id0 x:
id0 x:
id0 x:
id0 x:
id0 x:
id0 x:
id0 x:
Traceback (most recent call last):
  File "/mlbio_scratch/ekbote/Markov/Markov-LLM-depth/src/main.py", line 151, in <module>
    main(args)
  File "/mlbio_scratch/ekbote/Markov/Markov-LLM-depth/src/main.py", line 133, in main
    stats = train(model, opt, P, order, scheduler, args.iterations, args.acc_steps, args.batch_size, args.sequence_length, generator,
  File "/mlbio_scratch/ekbote/Markov/Markov-LLM-depth/src/optim/base.py", line 55, in train_base
    val_acc, val_loss, val_perplexity = eval(model, P, order, sequence_length, batch_size,
  File "/home/ekbote/.conda/envs/torch-2.4/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/mlbio_scratch/ekbote/Markov/Markov-LLM-depth/src/optim/utils.py", line 59, in eval
    x, y = get_batch(P, order, sequence_length, batch_size, generator, extra_args, device=device)
  File "/mlbio_scratch/ekbote/Markov/Markov-LLM-depth/src/optim/utils.py", line 37, in get_batch
    data[:,i] = get_next_symbols(P, order, data[:,i-order:i])
  File "/mlbio_scratch/ekbote/Markov/Markov-LLM-depth/src/optim/utils.py", line 47, in get_next_symbols
    s = torch.multinomial(M,1).flatten()
KeyboardInterrupt